Tramite l'uso combinato di Beautiful Soup 4 e Selenium, è stato effettuato un processo di web scraping, per raccogliere, da vari siti web a tema legale, circa 66.000 articoli, sotto forma di Q&A, scritti da esperti del settore in risposta alle domande e alle curiosità degli utenti. Gli articoli, raccolti in un dataset, sono stati successivamente pre-processati per rimuovere stopwords, commenti degli utenti, nomi degli autori, pubblicità, disclaimer e altre parole di scarso valore semantico. In questa fase iniziale, è stata utilizzata ampiamente la libreria re di Python. I documenti ripuliti sono stati poi vettorizzati utilizzando la tecnica del tf-idf-vectorizer. Alla lista di vettori ottenuti è stato applicato un clustering k-means con k=7. La scelta del numero di cluster ottimale è stata effettuata sulla base di un’analisi dell'SSE. Successivamente, per rendere più immediata la visualizzazione dei cluster, è stata applicata una PCA per ridurre a 2 la dimensione dei vettori iniziali. Una volta ottenuti i 7 cluster, si è posto il problema di assegnare, ad ognuno un’etichetta. Per farlo abbiamo stampato le parole chiave di ogni cluster e abbiamo scelto a mano un’etichetta che fosse riassuntiva delle tematiche principali.  